# -*- coding: utf-8 -*-
"""Fraud_Detection_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S2wNwxlqeXMqnjDk44AVbiraN_VrLlDS

# Import Statements
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# Loading the Dataset"""

# Load the dataset into pandas DataFrame
df = pd.read_csv('indian_fraud_dataset.csv')
df.head(3)

# Check the data types of each column
print(df.dtypes)

"""# EDA - Visualizations"""

# Check the distribution of fraud and non-fraud cases
print(df['is_fraud'].value_counts())

# Plot the class distribution
import matplotlib.pyplot as plt
df['is_fraud'].value_counts().plot(kind='bar', color=['green', 'red'])
plt.title('Class Distribution: Fraud vs Non-Fraud')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

import matplotlib.pyplot as plt

# Automatically select numeric columns
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Plot histograms
df[numeric_cols].hist(bins=20, figsize=(12, 10))
plt.suptitle('Distribution of Numerical Features')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,4))
sns.histplot(df['amount'], bins=100, log_scale=True, kde=True)
plt.title('Transaction Amount Distribution (log scale)')
plt.xlabel('Amount')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,4))
sns.boxplot(x='is_fraud', y='amount', data=df)
plt.yscale('log')
plt.title('Boxplot of Amount by Fraud Label')
plt.xlabel('Is Fraud')
plt.ylabel('Amount (log scale)')
plt.tight_layout()
plt.show()

df['timestamp'] = pd.to_datetime(df['timestamp'])  # Ensure 'timestamp' is datetime
df['transaction_hour'] = df['timestamp'].dt.hour  # Extract hour of the day


# Aggregate
hourly = df.groupby(['transaction_hour', 'is_fraud']).size().reset_index(name='count')
pivot = hourly.pivot(index='transaction_hour', columns='is_fraud', values='count').fillna(0)
pivot['fraud_rate'] = pivot[1] / (pivot[0] + pivot[1])

# Plot fraud rate by hour
plt.figure(figsize=(10,4))
sns.lineplot(x=pivot.index, y=pivot['fraud_rate'])
plt.title('Hourly Fraud Rate')
plt.xlabel('Hour of Day')
plt.ylabel('Fraud Rate')
plt.xticks(range(0,24))
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
sns.countplot(x='transaction_type', hue='is_fraud', data=df, palette='Set2')
plt.title('Transaction Type Counts by Fraud Label')
plt.xlabel('Transaction Type')
plt.ylabel('Count')
plt.legend(title='Is Fraud')
plt.tight_layout()
plt.show()

"""# EDA - Feature Engineering"""

df.info()
df.describe()

# See unique counts and examples
for col in df.columns:
    print(f"{col} | Unique: {df[col].nunique()} | Sample: {df[col].unique()[:6]}")

df.nunique()

# Parse datetime if it's not parsed
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Create rich time-based features
df['hour'] = df['timestamp'].dt.hour
df['dayofweek'] = df['timestamp'].dt.dayofweek
df['day'] = df['timestamp'].dt.day
df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
df['is_night'] = df['hour'].apply(lambda x: 1 if x <= 6 or x >= 22 else 0)

df['hour_sin'] = np.sin(2 * np.pi * df['transaction_hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['transaction_hour'] / 24)

df['log_amount'] = np.log1p(df['amount'])
df['is_high_amount'] = (df['amount'] > df['amount'].quantile(0.95)).astype(int)

## Same location flag
df['same_location'] = (df['location_sender'] == df['location_receiver']).astype(int)

## Categorize time into periods
def time_period(hour):
    if 5 <= hour < 12:
        return 'morning'
    elif 12 <= hour < 17:
        return 'afternoon'
    elif 17 <= hour < 21:
        return 'evening'
    else:
        return 'night'

df['hour_category'] = df['transaction_hour'].apply(time_period)
df['hour_category'] = df['hour_category'].astype('category').cat.codes

# Create transaction counts per sender/receiver (do this before dropping those columns if needed)
sender_counts = df['sender_id'].value_counts().to_dict()
receiver_counts = df['receiver_id'].value_counts().to_dict()
df['sender_txn_count'] = df['sender_id'].map(sender_counts)
df['receiver_txn_count'] = df['receiver_id'].map(receiver_counts)

from sklearn.preprocessing import LabelEncoder

categorical_cols = ['transaction_type', 'device_type', 'location_sender',
                    'location_receiver', 'hour_category']

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])

# Drop high-cardinality columns
df.drop(['sender_id', 'receiver_id'], axis=1, inplace=True)
df.drop('timestamp', axis=1, inplace=True)

print(f"ðŸ§¾ Data Shape: {df.shape}")
df.head(3)

df.info()
df.isnull().sum().sort_values(ascending=False)

# Check for duplicate rows
duplicates = df.duplicated().sum()
print(f"Duplicate rows: {duplicates}")

"""# Outliers"""

from scipy.stats import zscore
# Identify numeric columns
num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()

# Compute zâ€‘scores for each numeric column
df_z = df[num_cols].apply(zscore)

# Flag any row that is an outlier in *any* numeric column
outlier_mask = (df_z.abs() > 3).any(axis=1)
df['any_numeric_outlier'] = outlier_mask

print("Rows with any numeric outlier:", df['any_numeric_outlier'].sum())

import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 4))
plt.boxplot(np.log1p(df['amount']), vert=False)  # log1p handles 0 safely
plt.title('Boxplot of Log(Transaction Amount)')
plt.xlabel('log(Amount + 1)')
plt.show()

from scipy.stats import zscore
from sklearn.preprocessing import RobustScaler

# 1. Flag perâ€‘col outliers
for col in ['amount']:
    df[f'{col}_z'] = zscore(df[col])
    df[f'{col}_outlier'] = df[f'{col}_z'].abs() > 3

# 2. Winsorize amount
low, high = df['amount'].quantile([0.01, 0.99])
df['amount_capped'] = df['amount'].clip(lower=low, upper=high)

# 3. Robust scale numeric features
scaler = RobustScaler()
nums = ['amount_capped', 'sender_txn_count', 'receiver_txn_count']
df[nums] = scaler.fit_transform(df[nums])

# Summary stats
df.describe(include='all').T

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(16, 10))
sns.heatmap(df.select_dtypes(include=np.number).corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("ðŸ”— Feature Correlation Matrix")
plt.show()

"""# Data Split & SMOTE Sampling"""

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from collections import Counter

# Step 1: Split the data into training and testing sets
X = df.drop(columns=['is_fraud'])  # Features
y = df['is_fraud']  # Target (fraud or not)

# Split data (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 2. Show original class distribution
print("Before SMOTE:", Counter(y_train))

# 3. Apply SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# 4. Show new class distribution
print("After SMOTE: ", Counter(y_train_res))

# (Optional) also print shapes
print(f"X_train shape: {X_train.shape} â†’ X_train_resampled shape: {X_train_res.shape}")
print(f"y_train shape: {y_train.shape} â†’ y_train_resampled shape: {y_train_res.shape}")

print(X_train.columns)

from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_train_res_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

"""# Applying IsolationForest"""

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import IsolationForest

iso_forest = IsolationForest(contamination=0.03, random_state=42)
y_pred_iso = iso_forest.fit_predict(X_test_scaled)

# Convert predictions: -1 = outlier, 1 = inlier â†’ fraud = 1, not fraud = 0
y_pred_iso = [1 if x == -1 else 0 for x in y_pred_iso]

print(classification_report(y_test, y_pred_iso))
print(confusion_matrix(y_test, y_pred_iso))

from sklearn.ensemble import IsolationForest

# Initialize Isolation Forest
iso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)

# Fit on training data only
iso_forest.fit(X_train_res_scaled)

# Generate anomaly scores (the lower, the more anomalous)
train_anomaly_scores = iso_forest.decision_function(X_train_res_scaled)
test_anomaly_scores = iso_forest.decision_function(X_test_scaled)

# Also get anomaly flags (1 = normal, -1 = outlier)
train_anomaly_flags = iso_forest.predict(X_train_res_scaled)
test_anomaly_flags = iso_forest.predict(X_test_scaled)

# Convert flags from (-1, 1) â†’ (1, 0) for easier interpretation
train_anomaly_flags = (train_anomaly_flags == -1).astype(int)
test_anomaly_flags = (test_anomaly_flags == -1).astype(int)

import numpy as np

# Stack the new features
X_train_with_iso = np.hstack((X_train_res_scaled,
                              train_anomaly_scores.reshape(-1, 1),
                              train_anomaly_flags.reshape(-1, 1)))

X_test_with_iso = np.hstack((X_test_scaled,
                             test_anomaly_scores.reshape(-1, 1),
                             test_anomaly_flags.reshape(-1, 1)))

print("New X_train shape:", X_train_with_iso.shape)
print("New X_test shape:", X_test_with_iso.shape)

"""# Modelling - Training & Evaluation

## Without using Isolation *Scores*
"""

!pip install catboost

# 1. Imports
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# 2. Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=100, class_weight='balanced'),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "LightGBM": LGBMClassifier(),
    "CatBoost": CatBoostClassifier(verbose=0)
}

# 3. Train & evaluate each
for name, model in models.items():
    print(f"\n=== {name} ===")
    # Fit on resampled, scaled training data
    model.fit(X_train_res_scaled, y_train_res)

    # Predict on scaled test data
    y_pred = model.predict(X_test_scaled)

    # Compute AUC if available
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test_scaled)[:, 1]
        print(f"AUC: {roc_auc_score(y_test, y_prob):.4f}")

    # Show precision, recall, F1
    print(classification_report(y_test, y_pred, digits=4))

"""## With Isolation Scores"""

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_fscore_support
import pandas as pd

# 1. Define your models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=100, class_weight='balanced'),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "LightGBM": LGBMClassifier()
}

# 2. Prepare your dataset with Isolation Forest features (30 features)
X_tr = X_train_with_iso  # Training data with Isolation Forest features
X_te = X_test_with_iso   # Test data with Isolation Forest features

# 3. Loop over each model
all_results = []

for name, model in models.items():
    # Fit the model
    model.fit(X_tr, y_train_res)
    # Make predictions
    y_pred = model.predict(X_te)
    y_prob = model.predict_proba(X_te)[:,1]
    # Calculate metrics
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred,
                                                                average='binary', zero_division=0)
    auc = roc_auc_score(y_test, y_prob)
    print(f"{name:20s}  AUC: {auc:.4f}  Precision: {precision:.4f}  Recall: {recall:.4f}  F1: {f1:.4f}")
    all_results.append({
        "Model": name,
        "AUC": auc,
        "Precision": precision,
        "Recall": recall,
        "F1": f1
    })

# 4. (Optional) Tabulate results for easy comparison
results_df = pd.DataFrame(all_results)
print("\n\n===== Summary Table =====")
print(results_df.round(4))

print(X_tr.shape)
print(X_te.shape)

feature_names = X_train.columns.tolist()
X_train_res_scaled_df = pd.DataFrame(X_train_res_scaled, columns=feature_names)
print(X_train_res_scaled_df.head())

X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_train.columns)
print(X_test_scaled_df.head())

# Pick one of the two, depending on what you want:
# Simply drop use_label_encoder completely
final_model = XGBClassifier(eval_metric='logloss', random_state=42)

final_model.fit(X_train_res_scaled, y_train_res)

from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Predict class labels
y_pred = final_model.predict(X_test_scaled)

# Predict probabilities for AUC
y_prob = final_model.predict_proba(X_test_scaled)[:, 1]

# 1. AUC
auc_score = roc_auc_score(y_test, y_prob)
print(f"\nAUC Score: {auc_score:.4f}")

# 2. Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

# 3. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["Not Fraud", "Fraud"], yticklabels=["Not Fraud", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

precision, recall, thresholds = precision_recall_curve(y_test, y_prob)

plt.plot(thresholds, precision[:-1], label='Precision')
plt.plot(thresholds, recall[:-1], label='Recall')
plt.xlabel("Threshold")
plt.legend()
plt.title("Precision-Recall vs Threshold")
plt.grid()
plt.show()

recall_target = 0.90
indices = np.where(recall[:-1] >= recall_target)[0]

if len(indices) > 0:
    best_idx = indices[np.argmax(precision[indices])]
    chosen_threshold = thresholds[best_idx]
    print(f"ðŸŽ¯ Best threshold for recall â‰¥ {recall_target}: {chosen_threshold:.4f}")
    print(f"â†’ Precision: {precision[best_idx]:.4f}, Recall: {recall[best_idx]:.4f}")
else:
    print(f"No threshold gives recall â‰¥ {recall_target}")

import numpy as np

f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)  # add epsilon to avoid division by 0
best_idx = np.argmax(f1_scores)

best_threshold = thresholds[best_idx]
print(f"ðŸ” Best threshold for highest F1: {best_threshold:.4f}")
print(f"â†’ Precision: {precision[best_idx]:.4f}, Recall: {recall[best_idx]:.4f}, F1: {f1_scores[best_idx]:.4f}")

y_pred_custom = (y_prob >= 0.0481).astype(int)

from sklearn.metrics import classification_report, confusion_matrix

print("Classification Report with Custom Threshold:")
print(classification_report(y_test, y_pred_custom, digits=4))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_custom))

xgb_iso = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_iso.fit(X_train_with_iso, y_train_res)
from sklearn.metrics import classification_report, roc_auc_score

y_pred_iso = xgb_iso.predict(X_test_with_iso)
y_prob_iso = xgb_iso.predict_proba(X_test_with_iso)[:, 1]

print("AUC:", roc_auc_score(y_test, y_prob_iso))
print(classification_report(y_test, y_pred_iso))

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
import matplotlib.pyplot as plt
import numpy as np

# 1. Train XGBoost on data WITH Isolation Forest features
xgb_iso = XGBClassifier(eval_metric='logloss')
xgb_iso.fit(X_train_with_iso, y_train_res)

# 2. Predict probabilities on test data (with Isolation Forest features)
y_prob_iso = xgb_iso.predict_proba(X_test_with_iso)[:, 1]

# 3. Optional: Plot Precision vs Recall vs Threshold
precision, recall, thresholds = precision_recall_curve(y_test, y_prob_iso)
plt.figure(figsize=(8, 5))
plt.plot(thresholds, precision[:-1], label='Precision')
plt.plot(thresholds, recall[:-1], label='Recall')
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision-Recall vs Threshold (with ISO features)")
plt.legend()
plt.grid(True)
plt.show()

# 4. Choose a low threshold (based on above graph or fixed)
low_threshold = 0.033
y_pred_custom_iso = (y_prob_iso >= low_threshold).astype(int)

# 5. Evaluate the performance
print(f"\n=== XGBoost + Isolation Forest + Threshold={low_threshold} ===")
print(f"AUC Score: {roc_auc_score(y_test, y_prob_iso):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_custom_iso, digits=4))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_custom_iso))

"""## After applying all sorts of techniques to increase recall, came up with two models

"""

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import joblib

# Train
xgb_no_iso = XGBClassifier(eval_metric='logloss')
xgb_no_iso.fit(X_train_res_scaled, y_train_res)

# Predict
y_prob_no_iso = xgb_no_iso.predict_proba(X_test_scaled)[:, 1]
y_pred_no_iso = (y_prob_no_iso >= 0.5).astype(int)

# Evaluate
print("=== XGBoost WITHOUT ISO ===")
print(f"AUC: {roc_auc_score(y_test, y_prob_no_iso):.4f}")
print(classification_report(y_test, y_pred_no_iso))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_no_iso))

# Save
joblib.dump(xgb_no_iso, "xgb_model_without_iso.pkl")

# Train
xgb_with_iso = XGBClassifier(eval_metric='logloss')
xgb_with_iso.fit(X_train_with_iso, y_train_res)

# Predict with low threshold
y_prob_with_iso = xgb_with_iso.predict_proba(X_test_with_iso)[:, 1]
y_pred_with_iso = (y_prob_with_iso >= 0.033).astype(int)

# Evaluate
print("\n=== XGBoost WITH ISO + Threshold=0.033 ===")
print(f"AUC: {roc_auc_score(y_test, y_prob_with_iso):.4f}")
print(classification_report(y_test, y_pred_with_iso))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_with_iso))

# Save
joblib.dump(xgb_with_iso, "xgb_model_with_iso_thresh_0033.pkl")

"""# SHAP - Explanability"""

import shap
import joblib

# Load model
xgb_no_iso = joblib.load("xgb_model_without_iso.pkl")

# Get explainer
explainer_no_iso = shap.TreeExplainer(xgb_no_iso)

# Compute SHAP values
shap_values_no_iso = explainer_no_iso.shap_values(X_test_scaled)

# Plot
print("=== SHAP Summary (Without ISO) ===")
shap.summary_plot(shap_values_no_iso, X_test_scaled, feature_names=X_train.columns.tolist())

# Load model
xgb_with_iso = joblib.load("xgb_model_with_iso_thresh_0033.pkl")

# Feature names (24 original + 2 new)
feature_names_with_iso = X_train.columns.tolist() + ['iso_score', 'iso_flag']

# Get explainer
explainer_with_iso = shap.TreeExplainer(xgb_with_iso)

# Compute SHAP values
shap_values_with_iso = explainer_with_iso.shap_values(X_test_with_iso)

# Plot
print("=== SHAP Summary (With ISO) ===")
shap.summary_plot(shap_values_with_iso, X_test_with_iso, feature_names=feature_names_with_iso)

import shap
import joblib

# Load model trained on 24 features
xgb_no_iso = joblib.load("xgb_model_without_iso.pkl")
explainer_no_iso = shap.TreeExplainer(xgb_no_iso)

# SHAP values on test set
shap_values_no_iso = explainer_no_iso.shap_values(X_test_scaled)

# Feature names
feature_names_no_iso = X_train.columns.tolist()

# Bar plot
print("=== SHAP Bar Plot (Without ISO Features) ===")
shap.summary_plot(shap_values_no_iso, X_test_scaled, feature_names=feature_names_no_iso, plot_type="bar")

# Load model trained on 26 features
xgb_with_iso = joblib.load("xgb_model_with_iso_thresh_0033.pkl")
explainer_with_iso = shap.TreeExplainer(xgb_with_iso)

# SHAP values on test set
shap_values_with_iso = explainer_with_iso.shap_values(X_test_with_iso)

# Feature names (24 + 2 Isolation Forest)
feature_names_with_iso = X_train.columns.tolist() + ['iso_score', 'iso_flag']

# Bar plot
print("=== SHAP Bar Plot (With ISO Features) ===")
shap.summary_plot(shap_values_with_iso, X_test_with_iso, feature_names=feature_names_with_iso, plot_type="bar")

"""# Feature Importance"""

import numpy as np
import pandas as pd
import shap

# Assumes: xgb_no_iso and shap_values_no_iso already computed
shap_importance_no_iso = np.abs(shap_values_no_iso).mean(axis=0)
feature_names_no_iso = X_train.columns.tolist()

feature_importance_no_iso = pd.DataFrame({
    'Feature': feature_names_no_iso,
    'Importance': shap_importance_no_iso
}).sort_values(by='Importance', ascending=False)

print("=== Top 15 Features (Without ISO) ===")
print(feature_importance_no_iso.head(15))

# Assumes: xgb_with_iso and shap_values_with_iso already computed
shap_importance_with_iso = np.abs(shap_values_with_iso).mean(axis=0)
feature_names_with_iso = X_train.columns.tolist() + ['iso_score', 'iso_flag']

feature_importance_with_iso = pd.DataFrame({
    'Feature': feature_names_with_iso,
    'Importance': shap_importance_with_iso
}).sort_values(by='Importance', ascending=False)

print("=== Top 15 Features (With ISO) ===")
print(feature_importance_with_iso.head(15))

import pandas as pd

# Create a DataFrame to describe all key assets
data = {
    "Purpose": [
        "Training features (without ISO)",
        "Training features (with ISO + 2 features)",
        "Training labels (resampled)",
        "Test features (without ISO)",
        "Test features (with ISO + 2 features)",
        "Test labels (true y)",
        "Prediction probabilities (without ISO)",
        "Prediction probabilities (with ISO)",
        "Final XGBoost model (without ISO)",
        "Final XGBoost model (with ISO + threshold)",
        "Threshold for high-recall fraud detection",
        "SHAP values (without ISO)",
        "SHAP values (with ISO)",
    ],
    "Variable Name": [
        "X_train_res_scaled",
        "X_train_with_iso",
        "y_train_res",
        "X_test_scaled",
        "X_test_with_iso",
        "y_test",
        "y_prob_no_iso",
        "y_prob_iso",
        "xgb_model_no_iso",
        "xgb_model_with_iso",
        "THRESHOLD = 0.033",
        "shap_values_no_iso",
        "shap_values_with_iso",
    ],
    "Notes": [
        "Scaled training set after SMOTE, no Isolation Forest features",
        "Scaled + ISO scores + ISO flags appended (26 features)",
        "Binary labels for SMOTE-resampled training data",
        "Test features with same scaling & features (24 cols)",
        "Test features with added Isolation Forest insights",
        "Ground truth (fraud or not) for model evaluation",
        "Used for AUC, classification report (vanilla model)",
        "Used for custom-threshold classification (recall â†‘)",
        "Baseline model, highest F1-score (XGBoost)",
        "Model using Isolation Forest, tuned threshold",
        "Chosen after threshold tuning for fraud recall",
        "Explains predictions of baseline model",
        "Explains predictions of enhanced model"
    ]
}

df_dashboard_summary = pd.DataFrame(data)
print(df_dashboard_summary)

"""# Saving the Model, Features , X_test &  y_test"""

# Save as CSV
df_dashboard_summary.to_csv("dashboard_variable_summary.csv", index=False)

import joblib
import os

os.makedirs("saved_models", exist_ok=True)

# Models
joblib.dump(xgb_no_iso, "saved_models/xgb_model_without_iso.pkl")
joblib.dump(xgb_with_iso, "saved_models/xgb_model_with_iso_thresh_0033.pkl")

# Test sets
joblib.dump(X_test_scaled, "saved_models/X_test_scaled.pkl")
joblib.dump(X_test_with_iso, "saved_models/X_test_with_iso.pkl")
joblib.dump(y_test, "saved_models/y_test.pkl")

print("âœ… Both models and their test data saved for dashboard use.")

import pandas as pd
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_fscore_support

# Define your model references and data
models_info = [
    {
        "Name": "XGBoost Without ISO",
        "Model": xgb_no_iso,
        "X_test": X_test_scaled,
        "Threshold": 0.5
    },
    {
        "Name": "XGBoost With ISO + Thresh=0.033",
        "Model": xgb_with_iso,
        "X_test": X_test_with_iso,
        "Threshold": 0.033
    }
]

# Store results
results = []

for info in models_info:
    model = info["Model"]
    X = info["X_test"]
    threshold = info["Threshold"]

    # Get probabilities
    y_prob = model.predict_proba(X)[:, 1]

    # Apply threshold
    y_pred = (y_prob >= threshold).astype(int)

    # Compute metrics
    auc = roc_auc_score(y_test, y_prob)
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')

    results.append({
        "Model": info["Name"],
        "Threshold": threshold,
        "AUC": round(auc, 4),
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "F1-Score": round(f1, 4)
    })

# Convert to DataFrame
df_metrics = pd.DataFrame(results)

# Save to CSV
df_metrics.to_csv("saved_models/metrics_summary.csv", index=False)
print("âœ… Saved metrics_summary.csv in 'saved_models/' folder.")

# Save dashboard summary
df_dashboard_summary.to_csv("saved_models/df_dashboard_summary.csv", index=False)

import os

os.listdir("saved_models")

from google.colab import files
files.download("saved_models/df_dashboard_summary.csv")
files.download("saved_models/xgb_model_without_iso.pkl")
files.download("saved_models/xgb_model_with_iso_thresh_0033.pkl")
files.download("saved_models/X_test_scaled.pkl")
files.download("saved_models/X_test_with_iso.pkl")
files.download("saved_models/y_test.pkl")
files.download("saved_models/metrics_summary.csv")

print("âœ… All artifacts saved successfully! Models, test data,metrics summary dnd dashboard summary are ready for integration with the Bokeh dashboard. ðŸš€")